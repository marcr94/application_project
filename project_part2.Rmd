---
title: "Application Part Big Data for Economists Seminar"
author: "Marc Richter and Jingyan Yang"
date: "25 4 2020"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Data Pre-Processing

## Set up work space

We tidy up our environment, load some packages that we are gonna use later on and load our train and test data.

```{r}

rm(list = ls())

library(rstudioapi)
library(mice)
library(tidyverse)
library(Hmisc)
library(randomForest)
library(doParallel)
library(caret)
library(glmnet)
library(MASS) 
library(leaps) 
library(tree)
library(corrplot)
library(splines)
library(foreach)
library(gam)
library(boot) 
library(ranger)

setwd(dirname(rstudioapi::getActiveDocumentContext()$path))

train_data <- read.csv("./training.csv")

X_test <- read.csv("./X_test.csv")


```

## Overview our data

Let's have a look at our data. We have no data set information/fact sheet so we will have to figure out the content ourselves...

```{r, echo=FALSE}

head(train_data)
summary(train_data)

```

We have the following columns:

* ID: just the row ID
* address: addresses of the homes . sometimes with street name, street number, sometimes only postcode and town... pretty useless. we have the zipcode in an additional column anyway
* area: should be the total built area of the home.
* area_useable: the area of the home including land
* date: date of advertisement release
* date available
* home_type , ca. 190 categories
* municipality . probably won't use, have post code
* newly_built
* price : our dependent variable in this exercise
* rooms : number of rooms
* street: street name, won't use
* year: year of advertisment release
* year_built
* zipcode
* number of dummy variables:
  + balcony
  + basemnt
  + bath_tube
  + building_plot
  + cabletv
  + ceiling
  + cheminee
  + elevator
  + first_time
  + furnished
  + kids_friendly
  + laundry
  + minergie
  + municipality
  + new building
  + oldbuilding
  + oven
  + parking_indoor
  + parking_outside
  + playground
  + pool
  + quiet
  + raisedgroundfloor
  + sale
  + sunny
  + terrace
  + topstorage
  + veranda
  + wheelchair


## Data Cleaning


```{r}
# code adapted from https://www.r-bloggers.com/imputing-missing-data-with-r-mice-package/ for easy missing proportion checking

pMiss <- function(x){sum(is.na(x))/length(x)*100}
apply(train_data,2, pMiss)
```

This looks very bad! 

Leaving out all rows with missing data is not an option, as it would leave us with pretty much no data, and additionally it would possible introduce bias - the data is most likely not missing completely at random.

One major problem: our dummy variable all only contain ones and NA's. This way, we have no way of knowing which of the NA's are actually NA's and which are 0s.

Approach: Coding all NA's in dummy vars to 0. New meaning of predictor: not "does have ..." but "characteristic ... is mentioned".

```{r}

dummy_cols <- c("balcony", "basement", "bath_tube", "building_plot", "cabletv", "ceiling", "cheminee", "elevator", "first_time", "furnished", "kids_friendly", "laundry", "minergie", "new_building", "oldbuilding", "oven", "parking_indoor", "parking_outside", "playground", "pool", "quiet", "raised_groundfloor", "sale", "sunny", "terrace", "topstorage", "veranda", "wheelchair")

train_without_dummyNA <- train_data %>% mutate_at(.vars = dummy_cols, .funs = funs(ifelse(is.na(.), 0, .)))

apply(train_without_dummyNA,2, pMiss)

```

Let us check for some specific numeric variables, if there are some significant outliers in the dataset.

```{r}
plot(train_without_dummyNA$area) # not real outliers,but when check manually some observations with area=1
plot(train_without_dummyNA$floors) # no real outliers
plot(train_without_dummyNA$price) # no real outliers
plot(train_without_dummyNA$rooms) # no real outliers
plot(train_without_dummyNA$year_built) # we could delete the obs. from 1200

train_without_dummyNA <- train_without_dummyNA %>% filter(is.na(year_built) | year_built >=1300) # deletes 4 obs.
```
The observations with area = 1 do not really make sense...but we have area = 1 obs. in the test data too, so we will keep them.

We delete 4 observations, which have values of year_built before 1300. They might distort our relationship, and we do not have observations with year_built  > 1300 in the test data, so it will be save to delete them.


```{r}

# var format transformations

train_without_dummyNA$price <- sub(pattern = "CHF", replacement = "", train_without_dummyNA$price)
train_without_dummyNA$price <- as.numeric(train_without_dummyNA$price) # price is a factor variable otherwise

# do not use zip code but first two digits of zip code -- otherwise too many clusters having too few observations

train_without_dummyNA$zipcode_11 <- ifelse(train_without_dummyNA$zipcode >= 1100 & train_without_dummyNA$zipcode <= 1199, 1, 0)
train_without_dummyNA$zipcode_12 <- ifelse(train_without_dummyNA$zipcode >= 1200 & train_without_dummyNA$zipcode <= 1299, 1, 0)
train_without_dummyNA$zipcode_13 <- ifelse(train_without_dummyNA$zipcode >= 1300 & train_without_dummyNA$zipcode <= 1399, 1, 0)
train_without_dummyNA$zipcode_14 <- ifelse(train_without_dummyNA$zipcode >= 1400 & train_without_dummyNA$zipcode <= 1499, 1, 0)
train_without_dummyNA$zipcode_15 <- ifelse(train_without_dummyNA$zipcode >= 1500 & train_without_dummyNA$zipcode <= 1599, 1, 0)
train_without_dummyNA$zipcode_16 <- ifelse(train_without_dummyNA$zipcode >= 1600 & train_without_dummyNA$zipcode <= 1699, 1, 0)
train_without_dummyNA$zipcode_17 <- ifelse(train_without_dummyNA$zipcode >= 1700 & train_without_dummyNA$zipcode <= 1799, 1, 0)
train_without_dummyNA$zipcode_18 <- ifelse(train_without_dummyNA$zipcode >= 1800 & train_without_dummyNA$zipcode <= 1899, 1, 0)
train_without_dummyNA$zipcode_19 <- ifelse(train_without_dummyNA$zipcode >= 1900 & train_without_dummyNA$zipcode <= 1999, 1, 0)
train_without_dummyNA$zipcode_20 <- ifelse(train_without_dummyNA$zipcode >= 2000 & train_without_dummyNA$zipcode <= 2099, 1, 0)
train_without_dummyNA$zipcode_21 <- ifelse(train_without_dummyNA$zipcode >= 2100 & train_without_dummyNA$zipcode <= 2199, 1, 0)
train_without_dummyNA$zipcode_22 <- ifelse(train_without_dummyNA$zipcode >= 2200 & train_without_dummyNA$zipcode <= 2299, 1, 0)
train_without_dummyNA$zipcode_23 <- ifelse(train_without_dummyNA$zipcode >= 2300 & train_without_dummyNA$zipcode <= 2399, 1, 0)
train_without_dummyNA$zipcode_24 <- ifelse(train_without_dummyNA$zipcode >= 2400 & train_without_dummyNA$zipcode <= 2499, 1, 0)
train_without_dummyNA$zipcode_25 <- ifelse(train_without_dummyNA$zipcode >= 2500 & train_without_dummyNA$zipcode <= 2599, 1, 0)
train_without_dummyNA$zipcode_26 <- ifelse(train_without_dummyNA$zipcode >= 2600 & train_without_dummyNA$zipcode <= 2699, 1, 0)
train_without_dummyNA$zipcode_27 <- ifelse(train_without_dummyNA$zipcode >= 2700 & train_without_dummyNA$zipcode <= 2799, 1, 0)
train_without_dummyNA$zipcode_28 <- ifelse(train_without_dummyNA$zipcode >= 2800 & train_without_dummyNA$zipcode <= 2899, 1, 0)
train_without_dummyNA$zipcode_29 <- ifelse(train_without_dummyNA$zipcode >= 2900 & train_without_dummyNA$zipcode <= 2999, 1, 0)
train_without_dummyNA$zipcode_30 <- ifelse(train_without_dummyNA$zipcode >= 3000 & train_without_dummyNA$zipcode <= 3099, 1, 0)
train_without_dummyNA$zipcode_31 <- ifelse(train_without_dummyNA$zipcode >= 3100 & train_without_dummyNA$zipcode <= 3199, 1, 0)
train_without_dummyNA$zipcode_32 <- ifelse(train_without_dummyNA$zipcode >= 3200 & train_without_dummyNA$zipcode <= 3299, 1, 0)
train_without_dummyNA$zipcode_33 <- ifelse(train_without_dummyNA$zipcode >= 3300 & train_without_dummyNA$zipcode <= 3399, 1, 0)
train_without_dummyNA$zipcode_34 <- ifelse(train_without_dummyNA$zipcode >= 3400 & train_without_dummyNA$zipcode <= 3499, 1, 0)
train_without_dummyNA$zipcode_35 <- ifelse(train_without_dummyNA$zipcode >= 3500 & train_without_dummyNA$zipcode <= 3599, 1, 0)
train_without_dummyNA$zipcode_36 <- ifelse(train_without_dummyNA$zipcode >= 3600 & train_without_dummyNA$zipcode <= 3699, 1, 0)
train_without_dummyNA$zipcode_37 <- ifelse(train_without_dummyNA$zipcode >= 3700 & train_without_dummyNA$zipcode <= 3799, 1, 0)
train_without_dummyNA$zipcode_38 <- ifelse(train_without_dummyNA$zipcode >= 3800 & train_without_dummyNA$zipcode <= 3899, 1, 0)
train_without_dummyNA$zipcode_39 <- ifelse(train_without_dummyNA$zipcode >= 3900 & train_without_dummyNA$zipcode <= 3999, 1, 0)
train_without_dummyNA$zipcode_40 <- ifelse(train_without_dummyNA$zipcode >= 4000 & train_without_dummyNA$zipcode <= 4099, 1, 0)
train_without_dummyNA$zipcode_41 <- ifelse(train_without_dummyNA$zipcode >= 4100 & train_without_dummyNA$zipcode <= 4199, 1, 0)
train_without_dummyNA$zipcode_42 <- ifelse(train_without_dummyNA$zipcode >= 4200 & train_without_dummyNA$zipcode <= 4299, 1, 0)
train_without_dummyNA$zipcode_43 <- ifelse(train_without_dummyNA$zipcode >= 4300 & train_without_dummyNA$zipcode <= 4399, 1, 0)
train_without_dummyNA$zipcode_44 <- ifelse(train_without_dummyNA$zipcode >= 4400 & train_without_dummyNA$zipcode <= 4499, 1, 0)
train_without_dummyNA$zipcode_45 <- ifelse(train_without_dummyNA$zipcode >= 4500 & train_without_dummyNA$zipcode <= 4599, 1, 0)
train_without_dummyNA$zipcode_46 <- ifelse(train_without_dummyNA$zipcode >= 4600 & train_without_dummyNA$zipcode <= 4699, 1, 0)
train_without_dummyNA$zipcode_47 <- ifelse(train_without_dummyNA$zipcode >= 4700 & train_without_dummyNA$zipcode <= 4799, 1, 0)
train_without_dummyNA$zipcode_48 <- ifelse(train_without_dummyNA$zipcode >= 4800 & train_without_dummyNA$zipcode <= 4899, 1, 0)
train_without_dummyNA$zipcode_49 <- ifelse(train_without_dummyNA$zipcode >= 4900 & train_without_dummyNA$zipcode <= 4999, 1, 0)
train_without_dummyNA$zipcode_50 <- ifelse(train_without_dummyNA$zipcode >= 5000 & train_without_dummyNA$zipcode <= 5099, 1, 0)
train_without_dummyNA$zipcode_51 <- ifelse(train_without_dummyNA$zipcode >= 5100 & train_without_dummyNA$zipcode <= 5199, 1, 0)
train_without_dummyNA$zipcode_52 <- ifelse(train_without_dummyNA$zipcode >= 5200 & train_without_dummyNA$zipcode <= 5299, 1, 0)
train_without_dummyNA$zipcode_53 <- ifelse(train_without_dummyNA$zipcode >= 5300 & train_without_dummyNA$zipcode <= 5399, 1, 0)
train_without_dummyNA$zipcode_54 <- ifelse(train_without_dummyNA$zipcode >= 5400 & train_without_dummyNA$zipcode <= 5499, 1, 0)
train_without_dummyNA$zipcode_55 <- ifelse(train_without_dummyNA$zipcode >= 5500 & train_without_dummyNA$zipcode <= 5599, 1, 0)
train_without_dummyNA$zipcode_56 <- ifelse(train_without_dummyNA$zipcode >= 5600 & train_without_dummyNA$zipcode <= 5699, 1, 0)
train_without_dummyNA$zipcode_57 <- ifelse(train_without_dummyNA$zipcode >= 5700 & train_without_dummyNA$zipcode <= 5799, 1, 0)
train_without_dummyNA$zipcode_58 <- ifelse(train_without_dummyNA$zipcode >= 5800 & train_without_dummyNA$zipcode <= 5899, 1, 0)
train_without_dummyNA$zipcode_59 <- ifelse(train_without_dummyNA$zipcode >= 5900 & train_without_dummyNA$zipcode <= 5999, 1, 0)
train_without_dummyNA$zipcode_60 <- ifelse(train_without_dummyNA$zipcode >= 6000 & train_without_dummyNA$zipcode <= 6099, 1, 0)
train_without_dummyNA$zipcode_61 <- ifelse(train_without_dummyNA$zipcode >= 6100 & train_without_dummyNA$zipcode <= 6199, 1, 0)
train_without_dummyNA$zipcode_62 <- ifelse(train_without_dummyNA$zipcode >= 6200 & train_without_dummyNA$zipcode <= 6299, 1, 0)
train_without_dummyNA$zipcode_63 <- ifelse(train_without_dummyNA$zipcode >= 6300 & train_without_dummyNA$zipcode <= 6399, 1, 0)
train_without_dummyNA$zipcode_64 <- ifelse(train_without_dummyNA$zipcode >= 6400 & train_without_dummyNA$zipcode <= 6499, 1, 0)
train_without_dummyNA$zipcode_65 <- ifelse(train_without_dummyNA$zipcode >= 6500 & train_without_dummyNA$zipcode <= 6599, 1, 0)
train_without_dummyNA$zipcode_66 <- ifelse(train_without_dummyNA$zipcode >= 6600 & train_without_dummyNA$zipcode <= 6699, 1, 0)
train_without_dummyNA$zipcode_67 <- ifelse(train_without_dummyNA$zipcode >= 6700 & train_without_dummyNA$zipcode <= 6799, 1, 0)
train_without_dummyNA$zipcode_68 <- ifelse(train_without_dummyNA$zipcode >= 6800 & train_without_dummyNA$zipcode <= 6899, 1, 0)
train_without_dummyNA$zipcode_69 <- ifelse(train_without_dummyNA$zipcode >= 6900 & train_without_dummyNA$zipcode <= 6999, 1, 0)
train_without_dummyNA$zipcode_70 <- ifelse(train_without_dummyNA$zipcode >= 7000 & train_without_dummyNA$zipcode <= 7099, 1, 0)
train_without_dummyNA$zipcode_71 <- ifelse(train_without_dummyNA$zipcode >= 7100 & train_without_dummyNA$zipcode <= 7199, 1, 0)
train_without_dummyNA$zipcode_72 <- ifelse(train_without_dummyNA$zipcode >= 7200 & train_without_dummyNA$zipcode <= 7299, 1, 0)
train_without_dummyNA$zipcode_73 <- ifelse(train_without_dummyNA$zipcode >= 7300 & train_without_dummyNA$zipcode <= 7399, 1, 0)
train_without_dummyNA$zipcode_74 <- ifelse(train_without_dummyNA$zipcode >= 7400 & train_without_dummyNA$zipcode <= 7499, 1, 0)
train_without_dummyNA$zipcode_75 <- ifelse(train_without_dummyNA$zipcode >= 7500 & train_without_dummyNA$zipcode <= 7599, 1, 0)
train_without_dummyNA$zipcode_76 <- ifelse(train_without_dummyNA$zipcode >= 7600 & train_without_dummyNA$zipcode <= 7699, 1, 0)
train_without_dummyNA$zipcode_77 <- ifelse(train_without_dummyNA$zipcode >= 7700 & train_without_dummyNA$zipcode <= 7799, 1, 0)
train_without_dummyNA$zipcode_78 <- ifelse(train_without_dummyNA$zipcode >= 7800 & train_without_dummyNA$zipcode <= 7899, 1, 0)
train_without_dummyNA$zipcode_79 <- ifelse(train_without_dummyNA$zipcode >= 7900 & train_without_dummyNA$zipcode <= 7999, 1, 0)
train_without_dummyNA$zipcode_80 <- ifelse(train_without_dummyNA$zipcode >= 8000 & train_without_dummyNA$zipcode <= 8099, 1, 0)
train_without_dummyNA$zipcode_81 <- ifelse(train_without_dummyNA$zipcode >= 8100 & train_without_dummyNA$zipcode <= 8199, 1, 0)
train_without_dummyNA$zipcode_82 <- ifelse(train_without_dummyNA$zipcode >= 8200 & train_without_dummyNA$zipcode <= 8299, 1, 0)
train_without_dummyNA$zipcode_83 <- ifelse(train_without_dummyNA$zipcode >= 8300 & train_without_dummyNA$zipcode <= 8399, 1, 0)
train_without_dummyNA$zipcode_84 <- ifelse(train_without_dummyNA$zipcode >= 8400 & train_without_dummyNA$zipcode <= 8499, 1, 0)
train_without_dummyNA$zipcode_85 <- ifelse(train_without_dummyNA$zipcode >= 8500 & train_without_dummyNA$zipcode <= 8599, 1, 0)
train_without_dummyNA$zipcode_86 <- ifelse(train_without_dummyNA$zipcode >= 8600 & train_without_dummyNA$zipcode <= 8699, 1, 0)
train_without_dummyNA$zipcode_87 <- ifelse(train_without_dummyNA$zipcode >= 8700 & train_without_dummyNA$zipcode <= 8799, 1, 0)
train_without_dummyNA$zipcode_88 <- ifelse(train_without_dummyNA$zipcode >= 8800 & train_without_dummyNA$zipcode <= 8899, 1, 0)
train_without_dummyNA$zipcode_89 <- ifelse(train_without_dummyNA$zipcode >= 8900 & train_without_dummyNA$zipcode <= 8999, 1, 0)
train_without_dummyNA$zipcode_90 <- ifelse(train_without_dummyNA$zipcode >= 9000 & train_without_dummyNA$zipcode <= 9099, 1, 0)
train_without_dummyNA$zipcode_91 <- ifelse(train_without_dummyNA$zipcode >= 9100 & train_without_dummyNA$zipcode <= 9199, 1, 0)
train_without_dummyNA$zipcode_92 <- ifelse(train_without_dummyNA$zipcode >= 9200 & train_without_dummyNA$zipcode <= 9299, 1, 0)
train_without_dummyNA$zipcode_93 <- ifelse(train_without_dummyNA$zipcode >= 9300 & train_without_dummyNA$zipcode <= 9399, 1, 0)
train_without_dummyNA$zipcode_94 <- ifelse(train_without_dummyNA$zipcode >= 9400 & train_without_dummyNA$zipcode <= 9499, 1, 0)
train_without_dummyNA$zipcode_95 <- ifelse(train_without_dummyNA$zipcode >= 9500 & train_without_dummyNA$zipcode <= 9599, 1, 0)
train_without_dummyNA$zipcode_96 <- ifelse(train_without_dummyNA$zipcode >= 9600 & train_without_dummyNA$zipcode <= 9699, 1, 0)
train_without_dummyNA$zipcode_97 <- ifelse(train_without_dummyNA$zipcode >= 9700 & train_without_dummyNA$zipcode <= 9799, 1, 0)
train_without_dummyNA$zipcode_98 <- ifelse(train_without_dummyNA$zipcode >= 9800 & train_without_dummyNA$zipcode <= 9899, 1, 0)
train_without_dummyNA$zipcode_99 <- ifelse(train_without_dummyNA$zipcode >= 9900 & train_without_dummyNA$zipcode <= 9999, 1, 0)

# check if there are no observations for zipcode dummy
check_zipcode_dummy <- colSums(train_without_dummyNA == 0)
which(check_zipcode_dummy == dim(train_without_dummyNA)[1]| check_zipcode_dummy == (dim(train_without_dummyNA)[1] -1))
train_without_dummyNA <- subset(train_without_dummyNA, select = -c(zipcode, zipcode_58, zipcode_59, zipcode_78, zipcode_79, zipcode_97, zipcode_98, zipcode_99)) # I also delete the original ZIPCODE variable here. 


# delete some useless or redundant variables
train_without_dummyNA <- subset(train_without_dummyNA, select = -c(id, # just like row name, useless
                                                                   address, # some spelling mistake, and we have zipcode
                                                                   area_useable, # too many missing values
                                                                   date, # date of ad publishment should not matter
                                                                   date_available, # too sparse information
                                                                   municipality, # we have zip code
                                                                   sale, # only contains 1's
                                                                   street)) # again, we use zip code

```

Now we cleaned quite some parts from the dataset . We still have area, floors, rooms and year_built with missing data. For these variables, we will use the mice package to impute them.


## Imputation Part


We use the mean method in the mice package for a first imputation. Mean imputation is a very simple and straight forward approach to imputing. The upside is the clear way of imputation, we are not going to create outliers and the mean of the variables does not get changed. On the other hand, the imputed values might be very different to the values that are the actual (unobserved) values. Also, even tho the mean stays the same, the variation is going to get lower, which is not what we might want to get a robust model. 
We use this as our baseline data set.

As a second reference model we use the cart method from mice, which uses classification and regression trees to impute the missing variablers. As we only have missing values that are continuous, regression trees are used to compute the imputed variables. This is going to give us very different values to the mean, creating a more variable data set.

We create one imputed data set each (m = 1), as multiple imputed data sets can only be used as a joint for model selection/prediciton when we can assume a linear relationship.

Furthermore we use maxit = 20 to make sure the predictors for the regression trees converge.


```{r}
## multiple core computation


# Counts the number of cores on your machine
K <- parallel::detectCores()

# This registers each core as a member of a
# computing cluster

cl <- makeCluster(K)

registerDoParallel(cl)


# using "mean" method from MICE package
  train_imputed_mean <- complete(mice(train_without_dummyNA, m=1, maxit = 20, meth = "mean", seed = 50))
write.csv(train_imputed_mean, file = "./train_imputed_mean.csv", row.names = FALSE)

# using "cart" method from MICE package
  train_imputed_cart <- complete(mice(train_without_dummyNA, m=1, maxit = 20, meth = "cart", seed = 50))
write.csv(train_imputed_cart, file = "./train_imputed_cart.csv", row.names = FALSE)

# Once you finished, you deactivate the cluster
# with these two commands.
stopCluster(cl)
registerDoSEQ()

```


## Compare the imputed datasets


First we compare our baseline and our reference data with the original train data (with the missing values). We use densitiy plots to find out if the distribution of variables we imputed has changed.

```{r}
train_imputed_mean <- read.csv("./train_imputed_mean.csv")
train_imputed_cart <- read.csv("./train_imputed_cart.csv")

# Look at the distribution of imputed numeric data
# area
par(mfrow=c(1,3))
plot(density(train_without_dummyNA$area, na.rm = TRUE), main = "area(dummyNA)")
plot(density(train_imputed_mean$area), main = "area(mean)", col = "red")
plot(density(train_imputed_cart$area), main = "area(cart)", col = "green")

# floors
par(mfrow=c(1,3))
plot(density(train_without_dummyNA$floors, na.rm = TRUE), main = "floors(dummyNA)")
plot(density(train_imputed_mean$floors), main = "floors(mean)", col = "red")
plot(density(train_imputed_cart$floors), main = "floors(cart)", col = "green")

# rooms
par(mfrow=c(1,3))
plot(density(train_without_dummyNA$rooms, na.rm = TRUE), main = "rooms(dummyNA)")
plot(density(train_imputed_mean$rooms), main = "rooms(mean)", col = "red")
plot(density(train_imputed_cart$rooms), main = "rooms(cart)", col = "green")

# year_built
par(mfrow=c(1,3))
plot(density(train_without_dummyNA$year_built, na.rm = TRUE), main = "year_built(dummyNA)")
plot(density(train_imputed_mean$year_built), main = "year_built(mean)", col = "red")
plot(density(train_imputed_cart$year_built), main = "year_built(cart)", col = "green")

```

The density functions show that the data is differently distributed between the data sets. The mean imputation leads to spikes at the mean value, and the cart imputatation leads to more evenly distributed data.
?????? weird with floors(Yes...using mean should not have so many spikes, but I think the new plots make sense right now? I made another density plot for each of numeric variable, and we can see that from these density plots, the cart imputation is more similar to the original dataset.)



We also compare the correlation matrices of the three data sets.

```{r}

train_forcorr <- cor(train_without_dummyNA %>% keep(is.numeric) %>% dplyr::select(-starts_with("zipcode")))
corrplot(train_forcorr, method = "circle", title = "Correlation Plot for train data with dummy NA's changed to 0, no imputation")


train_forcorr <- cor(train_imputed_mean %>% keep(is.numeric) %>% dplyr::select(-starts_with("zipcode")))
corrplot(train_forcorr, method = "circle", title = "Correlation Plot for mean imputation")

train_forcorr <- cor(train_imputed_cart %>% keep(is.numeric) %>% dplyr::select(-starts_with("zipcode")))
corrplot(train_forcorr, method = "circle", title = "Correlation Plot for cart imputation")

```

The correlation matrices look very similar and only differ very slightly between the two imputed data sets and the data without imputation.


# Modeling


## Preperational insights


We now wanna have a first look at which variables might influence our outcome variable and in which way. We take a look at graphs showing the bivariate relationship between different independent variables and our outcome variable, price.

```{r}

plot(train_imputed_mean$area, train_imputed_mean$price) # maybe delete area >600 , relationship seems pretty linear 

plot(train_imputed_mean$floors, train_imputed_mean$price) # hard to tell, but looks non linear

plot(train_imputed_mean$rooms, train_imputed_mean$price) # hard to tell too

plot(train_imputed_mean$year, train_imputed_mean$price) # no relationship detectable

plot(train_imputed_mean$year_built, train_imputed_mean$price) # quadratic/exponential ??



plot(train_imputed_cart$area, train_imputed_cart$price) # maybe delete area >600 # relationship seems pretty linear 

plot(train_imputed_cart$floors, train_imputed_cart$price) # hard to tellbut looks non linear

plot(train_imputed_cart$rooms, train_imputed_cart$price) # hard to tell too

plot(train_imputed_cart$year, train_imputed_cart$price) # no relationship detectable

plot(train_imputed_cart$year_built, train_imputed_cart$price) # quadratic/exponential ??
```


## Splitting our training data into training and validation partitions


```{r}

# Split the data into training and test set

set.seed(123)
training.samples <- train_imputed_mean$price %>%
  createDataPartition(p = 0.8, list = FALSE)
train.data  <- train_imputed_mean[training.samples, ]
test.data <- train_imputed_mean[-training.samples, ]

training.samples_reference <- train_imputed_cart$price %>%
  createDataPartition(p = 0.8, list = FALSE)
train.data_reference  <- train_imputed_cart[training.samples_reference, ]
test.data_reference <- train_imputed_cart[-training.samples_reference, ]

```


## Linear Model


We build a first basic linear model.

```{r}
# Build the model
lm_fit <- lm(price ~., data = train.data)

## test for influential observations via cooks distance
cooksd <- cooks.distance(lm_fit)

plot(cooksd, pch="*", cex=2, main="Influential Obs by Cooks distance")  # plot cook's distance
abline(h = 4*mean(cooksd, na.rm=T), col="green")  # add cutoff line
text(x=1:length(cooksd)+1, y=cooksd, labels=ifelse(cooksd>4*mean(cooksd, na.rm=T),names(cooksd),""), col="blue")  # add labels
influential <- as.numeric(names(cooksd)[(cooksd > 4*mean(cooksd, na.rm=T))])  # influential row numbers
head(train.data[influential, ])  # influential observations.

# delete influential data points

train.data_without_outliers <- train.data[-influential,] # Don't know why, when I ran this code, it rerurned me an error: Error in xj[i] : only 0's may be mixed with negative subscripts, which spent me the whole morning to deal with...but failed eventually...and I tried this again at noon, it is successfully running


# new model wihtout outliers

lm_fit_without_outliers <- lm(price ~., data = train.data_without_outliers)

# Make predictions and compute the R2, RMSE and MAE

predictions <- lm_fit %>% predict(test.data)
predictions_without_outliers <- lm_fit_without_outliers %>% predict(test.data)

# store MSE

lm_fit_mse <- (RMSE(predictions_without_outliers, test.data$price)^2)
lm_fit_without_outliers_mse <- (RMSE(predictions, test.data$price)^2)

data.frame( R2 = R2(predictions, test.data$price),
            RMSE = RMSE(predictions, test.data$price),
            MAE = MAE(predictions, test.data$price))

data.frame( R2 = R2(predictions_without_outliers, test.data$price),
            RMSE = RMSE(predictions_without_outliers, test.data$price),
            MAE = MAE(predictions_without_outliers, test.data$price))
            
# Check the residual vs fitted plot, patches of many positive residuals in the middle, but patches of negative residuals at the right-hand side, suggesting linear model maybe not appropriate.
plot(lm_fit)

```

Outliers do affect our model only arbitrarily. 

The plot of residual vs. plotted shows that there is heteroskedasticity present - our linear model assumptions are hurt. We will add some interaction terms first. 

```{r}
# Include interaction terms (area:rooms; floors:elvator; area:floors; area:zipcode)
lm_fit1 <- lm(price ~ . + area:rooms + floors:elevator + area:floors 
               + area:zipcode_11 + area:zipcode_12 + area:zipcode_13 + area:zipcode_14 + area:zipcode_15 
               + area:zipcode_16 + area:zipcode_17 + area:zipcode_18 + area:zipcode_19 + area:zipcode_20 
               + area:zipcode_21 + area:zipcode_22 + area:zipcode_23 + area:zipcode_24 + area:zipcode_25 
               + area:zipcode_26 + area:zipcode_27 + area:zipcode_28 + area:zipcode_29 + area:zipcode_30 
               + area:zipcode_31 + area:zipcode_32 + area:zipcode_33 + area:zipcode_34 + area:zipcode_35 
               + area:zipcode_36 + area:zipcode_37 + area:zipcode_38 + area:zipcode_39 + area:zipcode_40  
               + area:zipcode_41 + area:zipcode_42 + area:zipcode_43 + area:zipcode_44 + area:zipcode_45 
               + area:zipcode_46 + area:zipcode_47 + area:zipcode_48 + area:zipcode_49 + area:zipcode_50 
               + area:zipcode_51 + area:zipcode_52 + area:zipcode_53 + area:zipcode_54 + area:zipcode_55 
               + area:zipcode_56 + area:zipcode_57 + area:zipcode_60 + area:zipcode_61 + area:zipcode_62 
               + area:zipcode_63 + area:zipcode_64 + area:zipcode_65 + area:zipcode_66 + area:zipcode_67 
               + area:zipcode_68 + area:zipcode_69 + area:zipcode_70 + area:zipcode_71 + area:zipcode_72 
               + area:zipcode_73 + area:zipcode_74 + area:zipcode_75 + area:zipcode_76 + area:zipcode_77
               + area:zipcode_80 + area:zipcode_81 + area:zipcode_82 + area:zipcode_83 + area:zipcode_84 
               + area:zipcode_85 + area:zipcode_86 + area:zipcode_87 + area:zipcode_88 + area:zipcode_89 
               + area:zipcode_90 + area:zipcode_91 + area:zipcode_92 + area:zipcode_93 + area:zipcode_94 
               + area:zipcode_95 + area:zipcode_96, data = train.data) 


predictions_lm_fit1 <- lm_fit1 %>% predict(test.data)
data.frame( R2 = R2(predictions_lm_fit1, test.data$price),
            RMSE = RMSE(predictions_lm_fit1, test.data$price),
            MAE = MAE(predictions_lm_fit1, test.data$price))

# store mse

lm_fit1_mse <- (RMSE(predictions_lm_fit1, test.data$price)^2)

# including the interaction terms lowers RMSE to 411990.7.

plot(lm_fit1)
# there is some serious leverage from a couple data points 
```

The residual vs. fitted look better already - but still far from good ! We will now try out some polynomial terms in our regression.


```{r}
# have a try on polynomial regression
# First, check which degree of polynomial to choose
set.seed(1)
# for area
cv.error.area <- rep(0,5)
for (i in 1:5) {
  fit_area <- glm(price ~ . + area:rooms + floors:elevator + area:floors - area  + poly(area,df = i)
             + area:zipcode_11 + area:zipcode_12 + area:zipcode_13 + area:zipcode_14 + area:zipcode_15 
              + area:zipcode_16 + area:zipcode_17 + area:zipcode_18 + area:zipcode_19 + area:zipcode_20 
              + area:zipcode_21 + area:zipcode_22 + area:zipcode_23 + area:zipcode_24 + area:zipcode_25 
              + area:zipcode_26 + area:zipcode_27 + area:zipcode_28 + area:zipcode_29 + area:zipcode_30 
              + area:zipcode_31 + area:zipcode_32 + area:zipcode_33 + area:zipcode_34 + area:zipcode_35 
              + area:zipcode_36 + area:zipcode_37 + area:zipcode_38 + area:zipcode_39 + area:zipcode_40  
              + area:zipcode_41 + area:zipcode_42 + area:zipcode_43 + area:zipcode_44 + area:zipcode_45 
              + area:zipcode_46 + area:zipcode_47 + area:zipcode_48 + area:zipcode_49 + area:zipcode_50 
              + area:zipcode_51 + area:zipcode_52 + area:zipcode_53 + area:zipcode_54 + area:zipcode_55 
              + area:zipcode_56 + area:zipcode_57 + area:zipcode_60 + area:zipcode_61 + area:zipcode_62 
              + area:zipcode_63 + area:zipcode_64 + area:zipcode_65 + area:zipcode_66 + area:zipcode_67 
              + area:zipcode_68 + area:zipcode_69 + area:zipcode_70 + area:zipcode_71 + area:zipcode_72 
              + area:zipcode_73 + area:zipcode_74 + area:zipcode_75 + area:zipcode_76 + area:zipcode_77
              + area:zipcode_80 + area:zipcode_81 + area:zipcode_82 + area:zipcode_83 + area:zipcode_84 
              + area:zipcode_85 + area:zipcode_86 + area:zipcode_87 + area:zipcode_88 + area:zipcode_89 
              + area:zipcode_90 + area:zipcode_91 + area:zipcode_92 + area:zipcode_93 + area:zipcode_94 
              + area:zipcode_95 + area:zipcode_96, data = train.data)
  cv.error.area[i] <- cv.glm(train.data, fit_area, K = 5)$delta[1]
}
plot(cv.error.area, xlab = "Degrees of polynomial for area", ylab = "Cross-validation error", type = "b", col = "blue") # we set degrees of polynomial for area as 4

# for floors
cv.error.floors <- rep(0,5)
for (i in 1:5) {
  fit_floors <- glm(price ~ . + area:rooms + floors:elevator + area:floors - floors  + poly(floors,df = i)
             + area:zipcode_11 + area:zipcode_12 + area:zipcode_13 + area:zipcode_14 + area:zipcode_15 
              + area:zipcode_16 + area:zipcode_17 + area:zipcode_18 + area:zipcode_19 + area:zipcode_20 
              + area:zipcode_21 + area:zipcode_22 + area:zipcode_23 + area:zipcode_24 + area:zipcode_25 
              + area:zipcode_26 + area:zipcode_27 + area:zipcode_28 + area:zipcode_29 + area:zipcode_30 
              + area:zipcode_31 + area:zipcode_32 + area:zipcode_33 + area:zipcode_34 + area:zipcode_35 
              + area:zipcode_36 + area:zipcode_37 + area:zipcode_38 + area:zipcode_39 + area:zipcode_40  
              + area:zipcode_41 + area:zipcode_42 + area:zipcode_43 + area:zipcode_44 + area:zipcode_45 
              + area:zipcode_46 + area:zipcode_47 + area:zipcode_48 + area:zipcode_49 + area:zipcode_50 
              + area:zipcode_51 + area:zipcode_52 + area:zipcode_53 + area:zipcode_54 + area:zipcode_55 
              + area:zipcode_56 + area:zipcode_57 + area:zipcode_60 + area:zipcode_61 + area:zipcode_62 
              + area:zipcode_63 + area:zipcode_64 + area:zipcode_65 + area:zipcode_66 + area:zipcode_67 
              + area:zipcode_68 + area:zipcode_69 + area:zipcode_70 + area:zipcode_71 + area:zipcode_72 
              + area:zipcode_73 + area:zipcode_74 + area:zipcode_75 + area:zipcode_76 + area:zipcode_77
              + area:zipcode_80 + area:zipcode_81 + area:zipcode_82 + area:zipcode_83 + area:zipcode_84 
              + area:zipcode_85 + area:zipcode_86 + area:zipcode_87 + area:zipcode_88 + area:zipcode_89 
              + area:zipcode_90 + area:zipcode_91 + area:zipcode_92 + area:zipcode_93 + area:zipcode_94 
              + area:zipcode_95 + area:zipcode_96, data = train.data)
  cv.error.floors[i] <- cv.glm(train.data, fit_floors, K = 5)$delta[1]
}
plot(cv.error.floors, xlab = "Degrees of polynomial for floors", ylab = "Cross-validation error", type = "b", col = "blue") # we set degrees of polynomial for floors as 3

# for rooms
cv.error.rooms <- rep(0,5)
for (i in 1:5) {
  fit_rooms <- glm(price ~ . + area:rooms + floors:elevator + area:floors - rooms  + poly(rooms,df = i)
             + area:zipcode_11 + area:zipcode_12 + area:zipcode_13 + area:zipcode_14 + area:zipcode_15 
              + area:zipcode_16 + area:zipcode_17 + area:zipcode_18 + area:zipcode_19 + area:zipcode_20 
              + area:zipcode_21 + area:zipcode_22 + area:zipcode_23 + area:zipcode_24 + area:zipcode_25 
              + area:zipcode_26 + area:zipcode_27 + area:zipcode_28 + area:zipcode_29 + area:zipcode_30 
              + area:zipcode_31 + area:zipcode_32 + area:zipcode_33 + area:zipcode_34 + area:zipcode_35 
              + area:zipcode_36 + area:zipcode_37 + area:zipcode_38 + area:zipcode_39 + area:zipcode_40  
              + area:zipcode_41 + area:zipcode_42 + area:zipcode_43 + area:zipcode_44 + area:zipcode_45 
              + area:zipcode_46 + area:zipcode_47 + area:zipcode_48 + area:zipcode_49 + area:zipcode_50 
              + area:zipcode_51 + area:zipcode_52 + area:zipcode_53 + area:zipcode_54 + area:zipcode_55 
              + area:zipcode_56 + area:zipcode_57 + area:zipcode_60 + area:zipcode_61 + area:zipcode_62 
              + area:zipcode_63 + area:zipcode_64 + area:zipcode_65 + area:zipcode_66 + area:zipcode_67 
              + area:zipcode_68 + area:zipcode_69 + area:zipcode_70 + area:zipcode_71 + area:zipcode_72 
              + area:zipcode_73 + area:zipcode_74 + area:zipcode_75 + area:zipcode_76 + area:zipcode_77
              + area:zipcode_80 + area:zipcode_81 + area:zipcode_82 + area:zipcode_83 + area:zipcode_84 
              + area:zipcode_85 + area:zipcode_86 + area:zipcode_87 + area:zipcode_88 + area:zipcode_89 
              + area:zipcode_90 + area:zipcode_91 + area:zipcode_92 + area:zipcode_93 + area:zipcode_94 
              + area:zipcode_95 + area:zipcode_96, data = train.data)
  cv.error.rooms[i] <- cv.glm(train.data, fit_rooms, K = 5)$delta[1]
}
plot(cv.error.rooms, xlab = "Degrees of polynomial for rooms", ylab = "Cross-validation error", type = "b", col = "blue") # we set degrees of polynomial for rooms as 3

lm_fit2 <- lm (price ~ . -area - floors - rooms + area:rooms + floors:elevator + area:floors + poly(area, 4) + poly(floors, 3) +poly(rooms, 3)
              + area:zipcode_11 + area:zipcode_12 + area:zipcode_13 + area:zipcode_14 + area:zipcode_15 
              + area:zipcode_16 + area:zipcode_17 + area:zipcode_18 + area:zipcode_19 + area:zipcode_20 
              + area:zipcode_21 + area:zipcode_22 + area:zipcode_23 + area:zipcode_24 + area:zipcode_25 
              + area:zipcode_26 + area:zipcode_27 + area:zipcode_28 + area:zipcode_29 + area:zipcode_30 
              + area:zipcode_31 + area:zipcode_32 + area:zipcode_33 + area:zipcode_34 + area:zipcode_35 
              + area:zipcode_36 + area:zipcode_37 + area:zipcode_38 + area:zipcode_39 + area:zipcode_40  
              + area:zipcode_41 + area:zipcode_42 + area:zipcode_43 + area:zipcode_44 + area:zipcode_45 
              + area:zipcode_46 + area:zipcode_47 + area:zipcode_48 + area:zipcode_49 + area:zipcode_50 
              + area:zipcode_51 + area:zipcode_52 + area:zipcode_53 + area:zipcode_54 + area:zipcode_55 
              + area:zipcode_56 + area:zipcode_57 + area:zipcode_60 + area:zipcode_61 + area:zipcode_62 
              + area:zipcode_63 + area:zipcode_64 + area:zipcode_65 + area:zipcode_66 + area:zipcode_67 
              + area:zipcode_68 + area:zipcode_69 + area:zipcode_70 + area:zipcode_71 + area:zipcode_72 
              + area:zipcode_73 + area:zipcode_74 + area:zipcode_75 + area:zipcode_76 + area:zipcode_77
              + area:zipcode_80 + area:zipcode_81 + area:zipcode_82 + area:zipcode_83 + area:zipcode_84 
              + area:zipcode_85 + area:zipcode_86 + area:zipcode_87 + area:zipcode_88 + area:zipcode_89 
              + area:zipcode_90 + area:zipcode_91 + area:zipcode_92 + area:zipcode_93 + area:zipcode_94 
              + area:zipcode_95 + area:zipcode_96, data = train.data) 

predictions_lm_fit2 <- lm_fit2 %>% predict(test.data)

data.frame( R2 = R2(predictions_lm_fit2, test.data$price),
            RMSE = RMSE(predictions_lm_fit2, test.data$price),
            MAE = MAE(predictions_lm_fit2, test.data$price))

# store mse

lm_fit2_mse <- (RMSE(predictions_lm_fit2, test.data$price)^2)

```


This new fit including interaction terms and polynomials contains a lot of predictors, which might introduce some serious overfitting in the model. It likely contains many predictors that do not explain the data well. Because of this, we will perform automated stepwise model selection.


```{r, include = False}
# model selection
# backward selection: RMSE = 379297.9	
lm_backward <-  stepAIC(lm_fit2, direction = "backward")
predictions_lm_backward <- lm_backward %>% predict(test.data)
data.frame( R2 = R2(predictions_lm_backward, test.data$price),
            RMSE = RMSE(predictions_lm_backward, test.data$price),
            MAE = MAE(predictions_lm_backward, test.data$price))

# forward selection: RMSE = 379334.5
lm_start <- lm(price ~ 1, data = train.data)

lm_forward <-  stepAIC(lm_start, direction = "forward", scope = formula(lm_fit2))
predictions_lm_forward <- lm_forward %>% predict(test.data)
data.frame( R2 = R2(predictions_lm_forward, test.data$price),
            RMSE = RMSE(predictions_lm_forward, test.data$price),
            MAE = MAE(predictions_lm_forward, test.data$price))

# hybrid selection: RMSE = 379362.6
lm_hybrid <-  stepAIC(lm_start, direction = "both", scope = formula(lm_fit2))
predictions_lm_hybrid <- lm_hybrid %>% predict(test.data)
data.frame( R2 = R2(predictions_lm_hybrid, test.data$price),
            RMSE = RMSE(predictions_lm_hybrid, test.data$price),
            MAE = MAE(predictions_lm_hybrid, test.data$price))

# store mse: we choose backward selection since it gives us the minimum RMSE among these three models.
lm_backward_mse <- (RMSE(predictions_lm_backward, test.data$price)^2)
```


## Ridge, LASSO and Elastic-net regression

Our data has quite some variables. Especially as we redifined NA's to 0 in the case of dummy variables and imputed the other ones, this can introduce bias. Also, we do not want our model to get into the trap of what is known as "the curse of dimensionality". Therefore we choose to use Ridge and Lasso regressions by introducing the tuning parameter $\lambda$ to regularize our models. We can see that LASSO performs best among Ridge, LASSO and Elastic-Net regressions since it is designed to force coefficients to get exactly zero. This way, we might be able to drop some variables which do not have good explanational power.

```{r}
# need to find optimal alpha and lambda
# code from: https://github.com/StatQuest/ridge_lasso_elastic_net_demo/blob/master/ridge_lass_elastic_net_demo.R
# reform X (training and test) and Y training for lasso input
train_x <- model.matrix(price ~., train.data)[,-1]
train_y <- train.data$price

test_x <- model.matrix(price ~., test.data)[,-1]
test_y <- test.data$price

# alpha = 0, Ridge Regression; alpha = 1, Lasso; alpha = 0-1, combine ridge and lasso: Elastic-net regression
set.seed(1)
list.of.fits <- list()
for (i in 0:10) {
  fit.name <- paste0("alpha", i/10)
  
  list.of.fits[[fit.name]] <-
    cv.glmnet(train_x, train_y, type.measure="mse", alpha=i/10, 
      family="gaussian")
}

results <- data.frame()
for (i in 0:10) {
  fit.name <- paste0("alpha", i/10)
  
  predicted <- 
    predict(list.of.fits[[fit.name]], 
      s=list.of.fits[[fit.name]]$lambda.min, newx=test_x)
  
  mse <- mean((test_y - predicted)^2)
  
  temp <- data.frame(alpha=i/10, mse=mse, fit.name=fit.name)
  results <- rbind(results, temp)
}

# from results, we can see Lasso (alpha = 1) performs best among these three.
lasso_fit <- cv.glmnet(train_x, train_y, type.measure="mse", 
  alpha=1, family="gaussian")

lasso_predicted <- predict(lasso_fit, s=lasso_fit$lambda.min, newx = test_x)

lasso_mse <- mean((test_y - lasso_predicted)^2)

# Lasso with interaction terms: RMSE = 394129.6
train_x_1 <- model.matrix(price ~ .  + area:rooms + floors:elevator + area:floors 
              + area:zipcode_11 + area:zipcode_12 + area:zipcode_13 + area:zipcode_14 + area:zipcode_15 
              + area:zipcode_16 + area:zipcode_17 + area:zipcode_18 + area:zipcode_19 + area:zipcode_20 
              + area:zipcode_21 + area:zipcode_22 + area:zipcode_23 + area:zipcode_24 + area:zipcode_25 
              + area:zipcode_26 + area:zipcode_27 + area:zipcode_28 + area:zipcode_29 + area:zipcode_30 
              + area:zipcode_31 + area:zipcode_32 + area:zipcode_33 + area:zipcode_34 + area:zipcode_35 
              + area:zipcode_36 + area:zipcode_37 + area:zipcode_38 + area:zipcode_39 + area:zipcode_40  
              + area:zipcode_41 + area:zipcode_42 + area:zipcode_43 + area:zipcode_44 + area:zipcode_45 
              + area:zipcode_46 + area:zipcode_47 + area:zipcode_48 + area:zipcode_49 + area:zipcode_50 
              + area:zipcode_51 + area:zipcode_52 + area:zipcode_53 + area:zipcode_54 + area:zipcode_55 
              + area:zipcode_56 + area:zipcode_57 + area:zipcode_60 + area:zipcode_61 + area:zipcode_62 
              + area:zipcode_63 + area:zipcode_64 + area:zipcode_65 + area:zipcode_66 + area:zipcode_67 
              + area:zipcode_68 + area:zipcode_69 + area:zipcode_70 + area:zipcode_71 + area:zipcode_72 
              + area:zipcode_73 + area:zipcode_74 + area:zipcode_75 + area:zipcode_76 + area:zipcode_77
              + area:zipcode_80 + area:zipcode_81 + area:zipcode_82 + area:zipcode_83 + area:zipcode_84 
              + area:zipcode_85 + area:zipcode_86 + area:zipcode_87 + area:zipcode_88 + area:zipcode_89 
              + area:zipcode_90 + area:zipcode_91 + area:zipcode_92 + area:zipcode_93 + area:zipcode_94 
              + area:zipcode_95 + area:zipcode_96, data = train.data)[, -1]

test_x_1 <- model.matrix(price ~ .  + area:rooms + floors:elevator + area:floors 
              + area:zipcode_11 + area:zipcode_12 + area:zipcode_13 + area:zipcode_14 + area:zipcode_15 
              + area:zipcode_16 + area:zipcode_17 + area:zipcode_18 + area:zipcode_19 + area:zipcode_20 
              + area:zipcode_21 + area:zipcode_22 + area:zipcode_23 + area:zipcode_24 + area:zipcode_25 
              + area:zipcode_26 + area:zipcode_27 + area:zipcode_28 + area:zipcode_29 + area:zipcode_30 
              + area:zipcode_31 + area:zipcode_32 + area:zipcode_33 + area:zipcode_34 + area:zipcode_35 
              + area:zipcode_36 + area:zipcode_37 + area:zipcode_38 + area:zipcode_39 + area:zipcode_40  
              + area:zipcode_41 + area:zipcode_42 + area:zipcode_43 + area:zipcode_44 + area:zipcode_45 
              + area:zipcode_46 + area:zipcode_47 + area:zipcode_48 + area:zipcode_49 + area:zipcode_50 
              + area:zipcode_51 + area:zipcode_52 + area:zipcode_53 + area:zipcode_54 + area:zipcode_55 
              + area:zipcode_56 + area:zipcode_57 + area:zipcode_60 + area:zipcode_61 + area:zipcode_62 
              + area:zipcode_63 + area:zipcode_64 + area:zipcode_65 + area:zipcode_66 + area:zipcode_67 
              + area:zipcode_68 + area:zipcode_69 + area:zipcode_70 + area:zipcode_71 + area:zipcode_72 
              + area:zipcode_73 + area:zipcode_74 + area:zipcode_75 + area:zipcode_76 + area:zipcode_77
              + area:zipcode_80 + area:zipcode_81 + area:zipcode_82 + area:zipcode_83 + area:zipcode_84 
              + area:zipcode_85 + area:zipcode_86 + area:zipcode_87 + area:zipcode_88 + area:zipcode_89 
              + area:zipcode_90 + area:zipcode_91 + area:zipcode_92 + area:zipcode_93 + area:zipcode_94 
              + area:zipcode_95 + area:zipcode_96, data = test.data)[, -1]

lasso_fit_1 <- cv.glmnet(train_x_1, train_y, type.measure="mse", 
  alpha=1, family="gaussian")
lasso_predicted_1 <- predict(lasso_fit_1, s=lasso_fit_1$lambda.min, newx = test_x_1)
lasso_intreaction_mse <- mean((test_y - lasso_predicted_1)^2))
```



# Moving to nonlinearity: Smooth Spline and GAM
The plot of residual vs. plotted indicates that the linearity assumption might not hold for our dataset. Though we have tried out polynomial regressions before, we want to use more sophisticated approaches, like splines and generalized additive models. The reason is that smoothing splines are more flexible than polynomial and generalized additive midels allow us to apply the nonlinearity methods to multiple variables.


```{r}
# Smoothing spline for rooms
room.fit <- smooth.spline(train.data$rooms, train.data$price, cv=TRUE)
plot(train.data$rooms, train.data$price ,col =" gray ")
lines(room.fit ,col ="red",lwd =2)

floors.fit <- smooth.spline(train.data$floors, train.data$price, cv=TRUE)
plot(train.data$floors, train.data$price ,col =" gray ")
lines(floors.fit ,col ="red",lwd =2)

area.fit <- smooth.spline(train.data$area, train.data$price, cv=TRUE)
plot(train.data$area, train.data$price ,col =" gray ")
lines(area.fit, col ="red",lwd =2)

# GAM with smoothing splines
gam.m0 <- gam(price ~ . - area + s(area, area.fit$df) - floors + s(floors, floors.fit$df) - rooms + s(rooms, room.fit$df), data = train.data)
gam.m0_pred <- gam.m0 %>% predict(test.data)
data.frame( R2 = R2(gam.m0_pred, test.data$price),
            RMSE = RMSE(gam.m0_pred, test.data$price),
            MAE = MAE(gam.m0_pred, test.data$price))
gam_m0_mse <- ((RMSE(gam.m0_pred, test.data$price))^2)

# GAM with variables from backward selection and smoothing splines: RMSE = 381470.8	
gam.m1 <- gam(price ~ balcony + basement + bath_tube + cheminee + elevator + 
    home_type + kids_friendly + laundry + newly_built + oldbuilding + 
    oven + parking_indoor + parking_outside + pool + terrace + 
    topstorage + wheelchair + year + year_built + zipcode_11 + 
    zipcode_12 + zipcode_13 + zipcode_14 + zipcode_15 + zipcode_16 + 
    zipcode_17 + zipcode_18 + zipcode_19 + zipcode_20 + zipcode_21 + 
    zipcode_22 + zipcode_23 + zipcode_24 + zipcode_25 + zipcode_26 + 
    zipcode_27 + zipcode_28 + zipcode_29 + zipcode_30 + zipcode_31 + 
    zipcode_32 + zipcode_33 + zipcode_34 + zipcode_35 + zipcode_36 + 
    zipcode_37 + zipcode_38 + zipcode_39 + zipcode_41 + zipcode_42 + 
    zipcode_43 + zipcode_44 + zipcode_45 + zipcode_46 + zipcode_47 + 
    zipcode_48 + zipcode_49 + zipcode_50 + zipcode_51 + zipcode_52 + 
    zipcode_53 + zipcode_54 + zipcode_55 + zipcode_56 + zipcode_57 + 
    zipcode_60 + zipcode_61 + zipcode_62 + zipcode_63 + zipcode_64 + 
    zipcode_65 + zipcode_66 + zipcode_67 + zipcode_68 + zipcode_69 + 
    zipcode_70 + zipcode_71 + zipcode_72 + zipcode_73 + zipcode_74 + 
    zipcode_75 + zipcode_76 + zipcode_77 + zipcode_80 + zipcode_81 + 
    zipcode_82 + zipcode_83 + zipcode_84 + zipcode_85 + zipcode_86 + 
    zipcode_87 + zipcode_88 + zipcode_89 + zipcode_90 + zipcode_91 + 
    zipcode_92 + zipcode_93 + zipcode_94 + zipcode_95 + zipcode_96 + 
    area:rooms + s(area, area.fit$df) + s(floors, floors.fit$df) + s(rooms, room.fit$df) + 
    elevator:floors + area:floors + zipcode_11:area + zipcode_12:area + 
    zipcode_13:area + zipcode_14:area + zipcode_15:area + zipcode_16:area + 
    zipcode_17:area + zipcode_18:area + zipcode_19:area + zipcode_20:area + 
    zipcode_21:area + zipcode_22:area + zipcode_23:area + zipcode_24:area + 
    zipcode_25:area + zipcode_26:area + zipcode_27:area + zipcode_28:area + 
    zipcode_29:area + zipcode_30:area + zipcode_31:area + zipcode_32:area + 
    zipcode_33:area + zipcode_34:area + zipcode_35:area + zipcode_36:area + 
    zipcode_38:area + zipcode_39:area + zipcode_41:area + zipcode_42:area + 
    zipcode_43:area + zipcode_44:area + zipcode_45:area + zipcode_46:area + 
    zipcode_47:area + zipcode_48:area + zipcode_49:area + zipcode_50:area + 
    zipcode_51:area + zipcode_52:area + zipcode_53:area + zipcode_54:area + 
    zipcode_55:area + zipcode_56:area + zipcode_57:area + zipcode_60:area + 
    zipcode_61:area + zipcode_62:area + zipcode_63:area + zipcode_65:area + 
    zipcode_67:area + zipcode_68:area + zipcode_70:area + zipcode_71:area + 
    zipcode_73:area + zipcode_74:area + zipcode_75:area + zipcode_76:area + 
    zipcode_80:area + zipcode_81:area + zipcode_82:area + zipcode_83:area + 
    zipcode_84:area + zipcode_85:area + zipcode_86:area + zipcode_87:area + 
    zipcode_88:area + zipcode_89:area + zipcode_90:area + zipcode_91:area + 
    zipcode_92:area + zipcode_93:area + zipcode_94:area + zipcode_95:area + 
    zipcode_96:area, data = train.data)
gam.m1_pred <- gam.m1 %>% predict(test.data)
data.frame( R2 = R2(gam.m1_pred, test.data$price),
            RMSE = RMSE(gam.m1_pred, test.data$price),
            MAE = MAE(gam.m1_pred, test.data$price))
gam_m1_mse <- ((RMSE(gam.m1_pred, test.data$price))^2)

# Lasso with smoothing splines: RMSE = 394168
train_x_2 <- model.matrix(price ~ .  - area + s(area, area.fit$df) - floors + s(floors, floors.fit$df) 
              - rooms + s(rooms, room.fit$df) + area:rooms + floors:elevator + area:floors 
              + area:zipcode_11+ area:zipcode_12 + area:zipcode_13 + area:zipcode_14 + area:zipcode_15 
              + area:zipcode_16 + area:zipcode_17 + area:zipcode_18 + area:zipcode_19 + area:zipcode_20 
              + area:zipcode_21 + area:zipcode_22 + area:zipcode_23 + area:zipcode_24 + area:zipcode_25 
              + area:zipcode_26 + area:zipcode_27 + area:zipcode_28 + area:zipcode_29 + area:zipcode_30 
              + area:zipcode_31 + area:zipcode_32 + area:zipcode_33 + area:zipcode_34 + area:zipcode_35 
              + area:zipcode_36 + area:zipcode_37 + area:zipcode_38 + area:zipcode_39 + area:zipcode_40  
              + area:zipcode_41 + area:zipcode_42 + area:zipcode_43 + area:zipcode_44 + area:zipcode_45 
              + area:zipcode_46 + area:zipcode_47 + area:zipcode_48 + area:zipcode_49 + area:zipcode_50 
              + area:zipcode_51 + area:zipcode_52 + area:zipcode_53 + area:zipcode_54 + area:zipcode_55 
              + area:zipcode_56 + area:zipcode_57 + area:zipcode_60 + area:zipcode_61 + area:zipcode_62 
              + area:zipcode_63 + area:zipcode_64 + area:zipcode_65 + area:zipcode_66 + area:zipcode_67 
              + area:zipcode_68 + area:zipcode_69 + area:zipcode_70 + area:zipcode_71 + area:zipcode_72 
              + area:zipcode_73 + area:zipcode_74 + area:zipcode_75 + area:zipcode_76 + area:zipcode_77
              + area:zipcode_80 + area:zipcode_81 + area:zipcode_82 + area:zipcode_83 + area:zipcode_84 
              + area:zipcode_85 + area:zipcode_86 + area:zipcode_87 + area:zipcode_88 + area:zipcode_89 
              + area:zipcode_90 + area:zipcode_91 + area:zipcode_92 + area:zipcode_93 + area:zipcode_94 
              + area:zipcode_95 + area:zipcode_96, data = train.data)[, -1]


test_x_2 <- model.matrix(price ~ .  - area + s(area, area.fit$df) - floors + s(floors, floors.fit$df) 
              - rooms + s(rooms, room.fit$df) + area:rooms + floors:elevator + area:floors 
              + area:zipcode_11 + area:zipcode_12 + area:zipcode_13 + area:zipcode_14 + area:zipcode_15 
              + area:zipcode_16 + area:zipcode_17 + area:zipcode_18 + area:zipcode_19 + area:zipcode_20 
              + area:zipcode_21 + area:zipcode_22 + area:zipcode_23 + area:zipcode_24 + area:zipcode_25 
              + area:zipcode_26 + area:zipcode_27 + area:zipcode_28 + area:zipcode_29 + area:zipcode_30 
              + area:zipcode_31 + area:zipcode_32 + area:zipcode_33 + area:zipcode_34 + area:zipcode_35 
              + area:zipcode_36 + area:zipcode_37 + area:zipcode_38 + area:zipcode_39 + area:zipcode_40  
              + area:zipcode_41 + area:zipcode_42 + area:zipcode_43 + area:zipcode_44 + area:zipcode_45 
              + area:zipcode_46 + area:zipcode_47 + area:zipcode_48 + area:zipcode_49 + area:zipcode_50 
              + area:zipcode_51 + area:zipcode_52 + area:zipcode_53 + area:zipcode_54 + area:zipcode_55 
              + area:zipcode_56 + area:zipcode_57 + area:zipcode_60 + area:zipcode_61 + area:zipcode_62 
              + area:zipcode_63 + area:zipcode_64 + area:zipcode_65 + area:zipcode_66 + area:zipcode_67 
              + area:zipcode_68 + area:zipcode_69 + area:zipcode_70 + area:zipcode_71 + area:zipcode_72 
              + area:zipcode_73 + area:zipcode_74 + area:zipcode_75 + area:zipcode_76 + area:zipcode_77
              + area:zipcode_80 + area:zipcode_81 + area:zipcode_82 + area:zipcode_83 + area:zipcode_84 
              + area:zipcode_85 + area:zipcode_86 + area:zipcode_87 + area:zipcode_88 + area:zipcode_89 
              + area:zipcode_90 + area:zipcode_91 + area:zipcode_92 + area:zipcode_93 + area:zipcode_94 
              + area:zipcode_95 + area:zipcode_96, data = test.data)[, -1]

lasso_fit_2 <- cv.glmnet(train_x_2, train_y, type.measure="mse", 
  alpha=1, family="gaussian")
lasso_predicted_2 <- predict(lasso_fit_2, s=lasso_fit_2$lambda.min, newx = test_x_2)
lasso_gam_mse <- mean((test_y - lasso_predicted_2)^2)
```


Our lasso model has the advantage that there is only 15 predictors left which are used for computing the estimates. Also , our LASSO Model has the best RMSE so far. 

## Walking into the forest for some qualitree times
We can see that the regression tree is not competitive with other supervised learning approaches that shown before. The plot tree with node labels suggests that the regression tree only considers two variables, area and zipcode, and has only 7 terminal nodes. Consequently, its prediction accuracy is pretty low. In order to producee a more powerful prediction model, we also use random forest approach.
```{r}
# build a regression tree: 478696.5
set.seed(1)

tree_fit <- tree(price ~. , data = train.data)
cv_tree <- cv.tree(tree_fit)
tree_testyhat <- predict(tree_fit, newdata = test.data)
tree_mse <- mean((test.data$price - tree_testyhat)^2)
plot(tree_fit)
text(tree_fit, pretty = 0)


# random forest: RMSE = 396742.6, much better:)))
=======
# Random forest

rf <- ranger(price ~., data = train.data, write.forest = TRUE)
rf_testyhat <- predict(rf, test.data)$predictions
rf_mse <- mean((test.data$price - rf_testyhat)^2)
```


# Compare Models

```{r}
mse_vector <- c(lm_fit_mse, lm_fit_without_outliers_mse, lm_fit1_mse, lm_fit2_mse, lasso_mse, gam_m0_mse,tree_mse, rf_mse)
mse_table <- data.frame(mse_vector, row.names = c("Linear fit", "Linear fit without outliers", "Linear fit with interactions", "Linear fit with interactions and polynomials", "Lasso", "GAM", "Tree", "Random Forest"))
mse_table[,2] <- mse_table[1]/min(mse_vector)
colnames(mse_table) <- c("MSE", "difference to best model")
mse_table
```

## Imputation of the test dataset .

```{r}
# substitute NA's in dummy vars with 0
X_test_without_dummyNA <- X_test %>% mutate_at(.vars = dummy_cols, .funs = funs(ifelse(is.na(.), 0, .)))

# delete some useless or redundant variables
X_test_imputed <- subset(X_test_without_dummyNA, select = -c(id, address, area_useable, date, date_available, municipality, street))

apply(X_test_without_dummyNA,2, pMiss)

summary(X_test_without_dummyNA)
## multiple core computation


# Counts the number of cores on your machine
K <- parallel::detectCores()

# This registers each core as a member of a
# computing cluster

cl <- makeCluster(K)

registerDoParallel(cl)

# using "pmm" method from MICE package, as it needs quite a lot computation we set m=1 and maxit=1
  X_test_imputed_pmm <- complete(mice(X_test_imputed, m=2, maxit = 5, meth = "pmm", seed = 50), 1) 
write.csv(X_test_imputed_pmm, file = "./X_test_imputed_pmm.csv", row.names = FALSE)

# using "rf" method from MICE package
  train_imputed_rf <- complete(mice(train_imputed, m=2, maxit = 5, meth = "rf", seed = 50), 1) 
write.csv(train_imputed_rf, file = "./train_imputed_rf.csv", row.names = FALSE)
#apply(train_imputation_rf,2, pMiss)

# Once you finished, you deactivate the cluster
# with these two commands.
stopCluster(cl)
registerDoSEQ()

```
